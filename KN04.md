# KN04 - Containers in Action & Orchestration

## 1. Teil-Challange - Docker Image aufsetzen, in Registry ablegen und deployen - OCI: BASIC WORKFLOW

### 1. Schritt - Repository klonen

Zuerst habe ich das GitLab-Repository container-bootstrap geklont und bin in das Unterverzeichnis 01_container gewechselt:

![alt text](/Bilder/image_160.png)

### 2. Schritt - CSS-Hintergrundfarbe auf Goldgelb Ã¤ndern

In der Datei static/css/main.css habe ich den Hintergrundfarbwert des Web-App-Textblocks auf Goldgelb #ebd63d angepasst:

![alt text](/Bilder/image_161.png)


### 3. Schritt - Port in app.js auf 8091 Ã¤ndern

Ich habe den Port in der Datei app.js von Standard-Port 3000 auf den geforderten Port 8091 gesetzt:

![alt text](/Bilder/image_162.png)


### 4. Schritt - Bild â€Modul 169â€œ einfÃ¼gen

Ich habe die bestehende image.png durch das neue Bild image-new.png ersetzt (mit Titel: Modul 169):

![alt text](/Bilder/image_177.png)


### 5. Schritt - Port im Dockerfile freigeben

Am Ende des Dockerfiles habe ich den Port 8091 explizit mit EXPOSE freigegeben, damit er vom Container verwendet werden kann:

![alt text](/Bilder/image_163.png)


### 6. Schritt - Login in die GitLab Container Registry

Ich habe mich erfolgreich Ã¼ber die Kommandozeile bei registry.gitlab.com eingeloggt, um spÃ¤ter Images zu pushen:

![alt text](/Bilder/image_164.png)

### 7. Schritt - Docker-Image lokal bauen

Ich habe das Docker-Image lokal mit folgendem Namen und Tag erstellt:

`registry.gitlab.com/luka_vukadin/container-bootstrap/webapp_luka_vukadin_8091:1.0`

![alt text](/Bilder/image_166.png)

### 8. Schritt - Image in die GitLab Registry pushen

Nach erfolgreichem Build habe ich das Image in die GitLab Container Registry hochgeladen:

![alt text](/Bilder/image_167.png)

### 9. Schritt - Container von Registry starten

Ich habe den Container mit dem gepushten Image gestartet und den Port 8091 freigegeben:

![alt text](/Bilder/image_169.png)

### 10. Schritt - EC2-Sicherheitsgruppe anpassen

In der AWS-Konsole habe ich in der EC2-Sicherheitsgruppe eine neue Inbound-Regel fÃ¼r Port 8091 (TCP) hinzugefÃ¼gt, damit die App im Browser erreichbar ist:

![alt text](/Bilder/image_170.png)


### 11. Schritt - Container-Status prÃ¼fen

Ich habe mittels `docker ps` geprÃ¼ft, ob der Container erfolgreich lÃ¤uft:

![alt text](/Bilder/image_171.png)

### 12. Schritt - WebApp im Browser testen

Ich konnte die WebApp erfolgreich Ã¼ber folgenden Link im Browser erreichen: http://3.93.145.92:8091

![alt text](/Bilder/image_178.png)

### 13. Schritt - Container stoppen und lÃ¶schen

Nach dem Test habe ich den laufenden Container gestoppt und gelÃ¶scht:

![alt text](/Bilder/image_173.png)

### 14. Schritt - Image lokal lÃ¶schen

Abschliessend habe ich das Docker-Image lokal von der EC2-Instanz entfernt:

![alt text](/Bilder/image_174.png)
![alt text](/Bilder/image_176.png)


### Fazit

Ich konnte erfolgreich:

- Eine WebApp in einem Docker-Image anpassen,
- dieses Image in GitLab speichern,
- einen Container daraus starten
- und die App Ã¼ber das Internet zugÃ¤nglich machen.

---

## 2. Teil-Challange - Docker Compose - Container Orchestrierung mit mehreren Services - CONTAINER MANAGEMENT: ENTRY-LEVEL

### 1. Schritt â€“ docker-compose.yml anpassen

In der Datei docker-compose.yml habe ich folgende Ã„nderungen vorgenommen:

- Der published Port wurde auf 5169 gesetzt.
- Der interne Port (target) wurde auf 8080 gesetzt.
- Das Volume habe ich vuk-vol genannt (nach den ersten drei Buchstaben meines Nachnamens).
- Das Netzwerk wurde vuk-net genannt.

![alt text](/Bilder/image_181.png)

### 2. Schritt â€“ Split-Bild hochladen

Ich habe ein Bild von Split lokal auf meinem PC gespeichert und dann mit scp auf meine Instanz ins Verzeichnis /static/images/ Ã¼bertragen. Das Bild habe ich in logo.png umbenannt.

![alt text](/Bilder/image_192.png)

### 3. Schritt â€“ HTML anpassen

In der Datei index.html habe ich das FCZ-Logo durch mein eigenes Bild von Split ersetzt. AuSSerdem habe ich den Text unter dem Bild wie folgt angepasst:

â€Klicke auf refresh, um der schÃ¶nsten Stadt der Welt ihre verdienten Credits zu geben.
Du hast Split bisher â€¦ Credits gegeben.â€œ

![alt text](/Bilder/image_190.png)

### 4. Schritt â€“ Docker Compose starten

Danach habe ich die Anwendung mit docker compose up --build -d gestartet. Dabei wurden beide Container (Web-Frontend und Redis) gebaut und erfolgreich gestartet.

![alt text](/Bilder/image_184.png)

### 5. Schritt â€“ Containerstatus Ã¼berprÃ¼fen

Mit docker ps habe ich Ã¼berprÃ¼ft, ob beide Container laufen â€“ und das war der Fall. Alles war â€Upâ€œ.

![alt text](/Bilder/image_185.png)

### 6. Schritt â€“ Sicherheitsgruppe in AWS anpassen

Damit ich von aussen auf die App zugreifen kann, habe ich in der EC2-Sicherheitsgruppe eine neue Regel hinzugefÃ¼gt. Port 5169 (TCP) wurde fÃ¼r alle IPs (0.0.0.0/0) freigegeben.

![alt text](/Bilder/image_187.png)

### 7. Schritt â€“ Web-App aufrufen

Ich konnte nun meine App erfolgreich Ã¼ber die URL `http://3.93.145.92:5169` erreichen. Dort wurde mein neues Split-Logo angezeigt und auch der aktualisierte Text war sichtbar. Der Refresh-ZÃ¤hler funktioniert ebenfalls korrekt.

![alt text](/Bilder/image_193.png)
![alt text](/Bilder/image_194.png)

### 8. Schritt â€“ Volume und Netzwerk inspizieren

Zur Kontrolle habe ich folgende Befehle verwendet:

````
docker volume inspect 02_compose_vuk-vol
docker network inspect 02_compose_vuk-net
````
Damit konnte ich sehen, wo genau das Volume auf dem Host gespeichert wird, und welche IP-Adressen die beiden Container im internen Docker-Netzwerk erhalten haben.

![alt text](/Bilder/image_195.png)
![alt text](/Bilder/image_196.png)

----

## 3. Teil-Challange - Docker Compose - Container Orchestrierung mit mehreren Services - CONTAINER MANAGEMENT: ENTRY-LEVEL

### 1. Schritt â€“ EC2-Instanzen vorbereiten

Ich habe fÃ¼nf EC2-Instanzen in der AWS Management Console erstellt:

- 3 fÃ¼r Manager-Nodes
- 2 fÃ¼r Worker-Nodes

Die Instanzen sind in verschiedenen Availability Zones verteilt, um Ausfallsicherheit zu gewÃ¤hrleisten.

Ich habe allen Instanzen dieselbe Security Group zugewiesen.

![alt text](/Bilder/image_202.png)

### 2. Schritt â€“ Cloud-Init Script eingebunden

Beim Erstellen der Instanzen habe ich das folgende Cloud-Init Script eingebunden.  Dieses wird beim Start der Instanz automatisch ausgefÃ¼hrt und richtet die Umgebung automatisch so ein, wie ich sie fÃ¼r mein Docker Swarm Cluster brauche.

````
#cloud-config
packages:
  - apt-transport-https
  - ca-certificates
  - curl
  - gnupg-agent
  - software-properties-common
write_files:
  - path: /etc/sysctl.d/enabled_ipv4_forwarding.conf
    content: |
      net.ipv4.conf.all.forwarding=1
groups:
  - docker
system_info:
  default_user:
    groups: [docker]
runcmd:
  - curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
  - add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
  - apt-get update -y
  - apt-get install -y docker-ce docker-ce-cli containerd.io
  - systemctl start docker
  - systemctl enable docker
  - apt-get install podman -y
  - systemctl start podman
  - systemctl enable podman
````

**Was macht das Script konkret?**

- Es installiert wichtige Tools wie curl, gnupg-agent, software-properties-common â€“ also alles, was man fÃ¼r die Paketverwaltung braucht.
- Dann wird der IPv4-Forwarding-Modus aktiviert, was nÃ¶tig ist, damit die Instanzen spÃ¤ter im Cluster korrekt miteinander kommunizieren kÃ¶nnen.
- Die Docker-Gruppe wird angelegt und der Standardnutzer (ubuntu) wird direkt dieser Gruppe hinzugefÃ¼gt â€“ so kann ich Docker-Befehle ohne sudo ausfÃ¼hren.
- Danach wird Docker in einer stabilen Version installiert, inklusive Container Runtime containerd.io.
- ZusÃ¤tzlich wird auch noch Podman installiert â€“ das ist ein alternatives Container-Tool, welches Docker-kompatibel ist.
- Zum Schluss werden beide Dienste (docker, podman) automatisch gestartet und so konfiguriert, dass sie bei jedem Neustart automatisch wieder laufen.


### 3. Schritt â€“ Docker Swarm initialisieren (Manager 1)

Ich habe mich via SSH auf den ersten Manager-Node verbunden und mit folgendem Befehl den Swarm initialisiert:

````
docker swarm init --advertise-addr <PRIVATE_IP_MANAGER_1>
````

![alt text](/Bilder/image_205.png)

### 4. Schritt â€“ Manager & Worker Nodes joinen

Anschliessend habe ich die anderen Nodes (Manager 2, Manager 3, Worker 1 und Worker 2) mit den jeweiligen Join-Tokens hinzugefÃ¼gt:

````
# Auf Manager 2 und 3
docker swarm join --token <MANAGER_TOKEN> <PRIVATE_IP_MANAGER_1>:2377
````

- Manager2
![alt text](/Bilder/image_213.png)

- Manager3 
![alt text](/Bilder/image_215.png)

````
# Auf Worker 1 und 2
docker swarm join --token <WORKER_TOKEN> <PRIVATE_IP_MANAGER_1>:2377
````

- Worker1
![alt text](/Bilder/image_216.png)

- Worker2
![alt text](/Bilder/image_217.png)


### 5. Schritt â€“ Manager-Nodes auf Drain setzen

Damit nur die Worker-Nodes Container zugewiesen bekommen, habe ich die Availability der Manager-Nodes auf Drain gesetzt. Das verhindert, dass sie produktive Container Ã¼bernehmen â€“ ein Best Practice in produktiven Umgebungen.

![alt text](/Bilder/image_218.png)

````
docker node update --availability drain ip-172-31-93-217
docker node update --availability drain ip-172-31-80-191
docker node update --availability drain ip-172-31-92-183
````

![alt text](/Bilder/image_219.png)


### 6. Schritt â€“ Node-Status Ã¼berprÃ¼fen

Auf dem ersten Manager habe ich den Cluster-Status Ã¼berprÃ¼ft:

````
docker node ls
````

Ergebnis:

- 3 Manager-Nodes (Leader + 2 Reachable)
- 2 Worker-Nodes (Status: Active)

Keine Nodes im Status Down oder Unreachable

![alt text](/Bilder/image_212.png)

---

## 4. Teil-Challange - Service Deployment & Skalierung mit Docker Swarm

### 1. Schritt â€“ Service bereitstellen

Bevor ich den Service erstellt habe habe ich mit docker node ls`` Ã¼berprÃ¼ft ob die manager und worker richtig komnfiguriert wurden von mir und dies war der fall GrÃ¼n sind die Manager und Blau die Worker:

![alt text](/Bilder/image_221.png)

Jetzt kann ich den Befehl ausfÃ¼hren

**Befehl:**
````
docker service create --name 169-web --replicas 5 -p 5010:8080 marcellocalisto/webapp_one:1.0
````

**ErklÃ¤rung:**

Dieser Befehl erstellt einen neuen Swarm-Service namens 169-web der:

- extern auf Port 5010 lauscht (fÃ¼r den Browser)
- intern auf Port 8080 im Container lÃ¤uft
- 5 Replikas des Containers startet

![alt text](/Bilder/image_226.png)

Zum zeigen das der Befehl nur auf einer Manager Instance lÃ¤uft, habe ich getestet ob es bei einem worker funktioniert und dies war nicht fer fall es kommt die meldung `this is not a swarm manager`:

![alt text](/Bilder/image_222.png)

### Schritt 2 â€“ Service mit docker service ps prÃ¼fen

**Befehl:**

````
docker service ps 169-web
````

**ErklÃ¤rung:**

Dieser Befehl zeigt die Task-Liste des Services:

- Welche Container wo laufen
- Ob sie â€Runningâ€œ, â€Failedâ€œ oder â€Shutdownâ€œ sind
- Die IDs der Container

![alt text](/Bilder/image_252.png)


### Schritt 3 â€“ AWS Security Group: Port 5010 Ã¶ffnen

**Ort:**
AWS EC2 Console â†’ Instanzen â†’ Sicherheitsgruppen â†’ docker-swarm-cluster

**Aktion:**
Folgende Inbound-Regel wurde hinzugefÃ¼gt:

| Typ              | Protokoll | Portbereich | Quelle    | Beschreibung                  |
| ---------------- | --------- | ----------- | --------- | ----------------------------- |
| Benutzerdef. TCP | TCP       | 5010        | 0.0.0.0/0 | Web-App Ã¶ffentlich zugÃ¤nglich |


**ErklÃ¤rung:**
Damit ich meine Web-App Ã¶ffentlich Ã¼ber http://<Public-IP>:5010 aufrufen kann, muss ich den Port 5010 in der Security Group explizit Ã¶ffnen kÃ¶nnen â€“ auf allen relevanten Instanzen (Manager + Worker). Ohne diese Regel bleibt der Port blockiert.

![alt text](/Bilder/image_224.png)


### Schritt 4 â€“ Web-App Ã¼ber den Browser aufrufen

**URL im Browser:**

````
http://<Public-IP>:5010`
````

**ErklÃ¤rung:**

Wenn ich die IP-Adresse eines Worker-Nodes verwende (z.â€¯B. 18.233.156.9:5010), wird die Anfrage an einen Container im Swarm weitergeleitet.
Beim Neuladen der Seite sieht man unterschiedliche Container-IDs â€“ das zeigt, dass ein Load Balancer aktiv ist.

1. Website ohne Reload
![alt text](/Bilder/image_240.png)

2. Website mit Reload
![alt text](/Bilder/image_241.png)


### Schritt 5 â€“ Service auf 10 Container skalieren

**Befehl:**
````
docker service scale 169-web=10
````

**ErklÃ¤rung:**

Mit diesem Befehl wird der bestehende Service auf 10 Container erweitert. Docker Swarm verteilt sie automatisch auf die verfÃ¼gbaren Worker-Nodes.

![alt text](/Bilder/image_242.png)


### Schritt 6 â€“ Verteilung prÃ¼fen

**Befehle:**

````
# Auf dem Manager-Node
docker service ps 169-web
````

````
# Auf beiden Worker-Nodes
docker container ls
````

**ErklÃ¤rung:**

Nach dem Skalieren des Services auf 10 Container zeigt docker service ps, dass die Tasks gleichmÃ¤ssig auf die beiden Worker-Nodes verteilt wurden.
Mit docker container ls auf jedem Worker sehen wir, dass auf jedem Node 5 Container laufen.


Zeigt alle 10 Container (Tasks) des Services 169-web und auf welchen Nodes sie laufen.

- 5 laufen auf ip-172-31-80-40
- 5 laufen auf ip-172-31-95-73

![alt text](/Bilder/image_243.png)


Zeigt die lokal laufenden Container auf Worker 1 â€“ genau 5 StÃ¼ck.

![alt text](/Bilder/image_244.png)


Zeigt die lokal laufenden Container auf Worker 2 â€“ ebenfalls 5 StÃ¼ck.

![alt text](/Bilder/image_245.png)


### Schritt 7 â€“ 3 Container manuell lÃ¶schen

**Auf einem Worker-Node:**
````
docker container rm -f <ID1> <ID2> <ID3>`
````

**ErklÃ¤rung:**

Ich habe drei Container gelÃ¶scht und sie wurden im Manager als Failed angezeigt:

![alt text](/Bilder/image_246.png)


### Schritt 8 â€“ Service lÃ¶schen und aufrÃ¤umen

**Befehl:**
````
docker service rm 169-web
````

![alt text](/Bilder/image_253.png)

**Dann prÃ¼fen mit:**

in manager
````
docker service ls
````
![alt text](/Bilder/image_254.png)

in Worker
````
docker container ls
````

![alt text](/Bilder/image_255.png)


**ErklÃ¤rung:**

Der komplette Service wird entfernt, inkl. aller laufenden Container. Danach sind keine aktiven Services oder Container mehr sichtbar.


- docker service rm erfolgreich
- docker service ls â†’ leer
- docker container ls auf beiden Workern â†’ leer


----

## 5. Teil-Challange - Service Deployment & Skalierung mit Docker Swarm

ğŸ³ Aufgabe E â€“ 5. Teil-Challenge: Docker Swarm Deklarativ (Advanced-Level)
ğŸ§  Ziel
In dieser Challenge habe ich eine declarative Deployment-Methode mit Docker Swarm eingesetzt. Ziel war es, eine Web-Anwendung mit 6 Replikaten im Cluster zu betreiben, Load Balancing, Self-Healing, Skalierung, und das Clean-up zu testen.

ğŸ§© Vorbereitung & Voraussetzungen
âœ… 5-Node-Cluster in AWS (3 Manager + 2 Worker)

âœ… Docker & Swarm auf allen Nodes initialisiert

âœ… Eigene Security Group angepasst (siehe unten)

âœ… Projektstruktur vorbereitet

ğŸ” Schritt 0 â€“ AWS-Sicherheitsregel anpassen
Damit die Web-App Ã¼ber den Browser erreichbar ist:

In der AWS Management Console:

EC2 > Security Groups > Inbound Rules

Regel hinzugefÃ¼gt:

Type: Custom TCP

Port Range: 5169

Source: Anywhere (0.0.0.0/0) oder deine IP

âœ… Dadurch war die Web-App spÃ¤ter unter http://<Public-IP>:5169 erreichbar.

ğŸ§± Schritt 1 â€“ compose.yml vorbereiten
yaml
Kopieren
Bearbeiten
version: '3.8'

networks:
  vuk-net:

services:
  web-fe:
    image: lukavukadin/swarm-stack:1.0
    command: python app.py
    deploy:
      replicas: 6
    ports:
      - target: 8169
        published: 5169
    networks:
      - vuk-net

  redis:
    image: redis:alpine
    networks:
      - vuk-net
ğŸ“Œ Ã„nderungen:

ğŸ” Neuer interner Port 8169, externer Port 5169

ğŸ”€ Neues Netzwerk vuk-net

ğŸ”¢ replicas: 6

âš™ï¸ Schritt 2 â€“ Dockerfile anpassen
Damit Flask auf dem richtigen Port lauscht, wurde der Dockerfile folgendermaÃŸen angepasst:

Dockerfile
Kopieren
Bearbeiten
FROM python:3.11-slim
WORKDIR /app
COPY . .
RUN pip install flask redis
EXPOSE 8169
CMD ["python", "app.py"]
ğŸŒ Schritt 3 â€“ HTML-Template anpassen
In templates/index.html:

html
Kopieren
Bearbeiten
<h1 class="logo">Diese neue Web-App von "Vukadin" lÃ¤uft in einem Container!</h1>
<h3>Klicke auf refresh, um Deine Credits zu erhÃ¶hen. Deine Credits sind aktuell bei <b>{{ count }}</b></h3>
ğŸ“¦ Schritt 4 â€“ Image bauen & pushen
bash
Kopieren
Bearbeiten
docker build -t lukavukadin/swarm-stack:1.0 .
docker push lukavukadin/swarm-stack:1.0
ğŸš€ Schritt 5 â€“ Stack deployen
bash
Kopieren
Bearbeiten
docker stack deploy -c compose.yml zaehler
Verifiziert mit:

bash
Kopieren
Bearbeiten
docker stack ls
docker stack services zaehler
docker stack ps zaehler
ğŸ” Schritt 6 â€“ Funktion Ã¼ber Browser testen
Public IP vom Manager kopiert

Aufgerufen mit:
http://<Public-IP>:5169

âœ”ï¸ Web-App erfolgreich geladen
âœ”ï¸ Inhalt korrekt angezeigt: Container-ID, Credits

ğŸ”„ Load Balancer Test
Browser mehrfach neu geladen (Refresh)

Container-ID hat sich bei jedem Refresh geÃ¤ndert
â†’ zeigt, dass die Requests gleichmÃ¤ÃŸig verteilt wurden

Beispiel:

ID 1: 406fa8accf03

ID 2: 0216df5fc700
â†’ Load Balancing aktiv

ğŸ” Schritt 7 â€“ Skalierung: 6 â†’ 4
Ã„nderung in compose.yml:

yaml
Kopieren
Bearbeiten
replicas: 4
Dann erneut deployed:

bash
Kopieren
Bearbeiten
docker stack deploy -c compose.yml zaehler
Verifiziert mit:

bash
Kopieren
Bearbeiten
docker stack services zaehler
docker stack ps zaehler
â†’ 2 Replikate entfernt
â†’ GleichmÃ¤ÃŸig Ã¼ber die Worker verteilt

ğŸ”¥ Schritt 8 â€“ Self-Healing Test
Einen Worker-Node in AWS gestoppt (z.â€¯B. ip-172-31-80-40)

Beobachtet mit:

bash
Kopieren
Bearbeiten
docker stack ps zaehler
âœ”ï¸ Docker Swarm hat ausgefallene Replikate automatisch auf verbleibenden Nodes neu gestartet

Begriff:
ğŸ“Œ Self-Healing = Automatische Wiederherstellung des â€Desired Stateâ€œ

ğŸ§¹ Schritt 9 â€“ Clean-up
bash
Kopieren
Bearbeiten
docker stack rm zaehler
BestÃ¤tigt mit:

bash
Kopieren
Bearbeiten
docker stack ls
docker container ls -a
âœ”ï¸ Alle Services, Netzwerke und Container wurden entfernt


![alt text](image_257.png)


![alt text](image_258.png)


![alt text](image_260.png)


![alt text](image_261.png)







### Schritt 1 â€“ 


### Schritt 2 â€“ 


### Schritt 3 â€“ 


### Schritt 4 â€“ 


### Schritt 5 â€“ 


### Schritt 6 â€“ 


### Schritt 7 â€“ 
