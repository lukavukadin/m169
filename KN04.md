# KN04 - Containers in Action & Orchestration

## 1. Teil-Challange - Docker Image aufsetzen, in Registry ablegen und deployen - OCI: BASIC WORKFLOW

### 1. Schritt - Repository klonen

Zuerst habe ich das GitLab-Repository container-bootstrap geklont und bin in das Unterverzeichnis 01_container gewechselt:

![alt text](/Bilder/image_160.png)

### 2. Schritt - CSS-Hintergrundfarbe auf Goldgelb ändern

In der Datei static/css/main.css habe ich den Hintergrundfarbwert des Web-App-Textblocks auf Goldgelb #ebd63d angepasst:

![alt text](/Bilder/image_161.png)


### 3. Schritt - Port in app.js auf 8091 ändern

Ich habe den Port in der Datei app.js von Standard-Port 3000 auf den geforderten Port 8091 gesetzt:

![alt text](/Bilder/image_162.png)


### 4. Schritt - Bild „Modul 169“ einfügen

Ich habe die bestehende image.png durch das neue Bild image-new.png ersetzt (mit Titel: Modul 169):

![alt text](/Bilder/image_177.png)


### 5. Schritt - Port im Dockerfile freigeben

Am Ende des Dockerfiles habe ich den Port 8091 explizit mit EXPOSE freigegeben, damit er vom Container verwendet werden kann:

![alt text](/Bilder/image_163.png)


### 6. Schritt - Login in die GitLab Container Registry

Ich habe mich erfolgreich über die Kommandozeile bei registry.gitlab.com eingeloggt, um später Images zu pushen:

![alt text](/Bilder/image_164.png)

### 7. Schritt - Docker-Image lokal bauen

Ich habe das Docker-Image lokal mit folgendem Namen und Tag erstellt:

`registry.gitlab.com/luka_vukadin/container-bootstrap/webapp_luka_vukadin_8091:1.0`

![alt text](/Bilder/image_166.png)

### 8. Schritt - Image in die GitLab Registry pushen

Nach erfolgreichem Build habe ich das Image in die GitLab Container Registry hochgeladen:

![alt text](/Bilder/image_167.png)

### 9. Schritt - Container von Registry starten

Ich habe den Container mit dem gepushten Image gestartet und den Port 8091 freigegeben:

![alt text](/Bilder/image_169.png)

### 10. Schritt - EC2-Sicherheitsgruppe anpassen

In der AWS-Konsole habe ich in der EC2-Sicherheitsgruppe eine neue Inbound-Regel für Port 8091 (TCP) hinzugefügt, damit die App im Browser erreichbar ist:

![alt text](/Bilder/image_170.png)


### 11. Schritt - Container-Status prüfen

Ich habe mittels `docker ps` geprüft, ob der Container erfolgreich läuft:

![alt text](/Bilder/image_171.png)

### 12. Schritt - WebApp im Browser testen

Ich konnte die WebApp erfolgreich über folgenden Link im Browser erreichen: http://3.93.145.92:8091

![alt text](/Bilder/image_178.png)

### 13. Schritt - Container stoppen und löschen

Nach dem Test habe ich den laufenden Container gestoppt und gelöscht:

![alt text](/Bilder/image_173.png)

### 14. Schritt - Image lokal löschen

Abschliessend habe ich das Docker-Image lokal von der EC2-Instanz entfernt:

![alt text](/Bilder/image_174.png)
![alt text](/Bilder/image_176.png)


### Fazit

Ich konnte erfolgreich:

- Eine WebApp in einem Docker-Image anpassen,
- dieses Image in GitLab speichern,
- einen Container daraus starten
- und die App über das Internet zugänglich machen.

---

## 2. Teil-Challange - Docker Compose - Container Orchestrierung mit mehreren Services - CONTAINER MANAGEMENT: ENTRY-LEVEL

### 1. Schritt – docker-compose.yml anpassen

In der Datei docker-compose.yml habe ich folgende Änderungen vorgenommen:

- Der published Port wurde auf 5169 gesetzt.
- Der interne Port (target) wurde auf 8080 gesetzt.
- Das Volume habe ich vuk-vol genannt (nach den ersten drei Buchstaben meines Nachnamens).
- Das Netzwerk wurde vuk-net genannt.

![alt text](/Bilder/image_181.png)

### 2. Schritt – Split-Bild hochladen

Ich habe ein Bild von Split lokal auf meinem PC gespeichert und dann mit scp auf meine Instanz ins Verzeichnis /static/images/ übertragen. Das Bild habe ich in logo.png umbenannt.

![alt text](/Bilder/image_192.png)

### 3. Schritt – HTML anpassen

In der Datei index.html habe ich das FCZ-Logo durch mein eigenes Bild von Split ersetzt. AuSSerdem habe ich den Text unter dem Bild wie folgt angepasst:

„Klicke auf refresh, um der schönsten Stadt der Welt ihre verdienten Credits zu geben.
Du hast Split bisher … Credits gegeben.“

![alt text](/Bilder/image_190.png)

### 4. Schritt – Docker Compose starten

Danach habe ich die Anwendung mit docker compose up --build -d gestartet. Dabei wurden beide Container (Web-Frontend und Redis) gebaut und erfolgreich gestartet.

![alt text](/Bilder/image_184.png)

### 5. Schritt – Containerstatus überprüfen

Mit docker ps habe ich überprüft, ob beide Container laufen – und das war der Fall. Alles war „Up“.

![alt text](/Bilder/image_185.png)

### 6. Schritt – Sicherheitsgruppe in AWS anpassen

Damit ich von aussen auf die App zugreifen kann, habe ich in der EC2-Sicherheitsgruppe eine neue Regel hinzugefügt. Port 5169 (TCP) wurde für alle IPs (0.0.0.0/0) freigegeben.

![alt text](/Bilder/image_187.png)

### 7. Schritt – Web-App aufrufen

Ich konnte nun meine App erfolgreich über die URL `http://3.93.145.92:5169` erreichen. Dort wurde mein neues Split-Logo angezeigt und auch der aktualisierte Text war sichtbar. Der Refresh-Zähler funktioniert ebenfalls korrekt.

![alt text](/Bilder/image_193.png)
![alt text](/Bilder/image_194.png)

### 8. Schritt – Volume und Netzwerk inspizieren

Zur Kontrolle habe ich folgende Befehle verwendet:

````
docker volume inspect 02_compose_vuk-vol
docker network inspect 02_compose_vuk-net
````
Damit konnte ich sehen, wo genau das Volume auf dem Host gespeichert wird, und welche IP-Adressen die beiden Container im internen Docker-Netzwerk erhalten haben.

![alt text](/Bilder/image_195.png)
![alt text](/Bilder/image_196.png)

----

## 3. Teil-Challange - Docker Compose - Container Orchestrierung mit mehreren Services - CONTAINER MANAGEMENT: ENTRY-LEVEL

### 1. Schritt – EC2-Instanzen vorbereiten

Ich habe fünf EC2-Instanzen in der AWS Management Console erstellt:

- 3 für Manager-Nodes
- 2 für Worker-Nodes

Die Instanzen sind in verschiedenen Availability Zones verteilt, um Ausfallsicherheit zu gewährleisten.

Ich habe allen Instanzen dieselbe Security Group zugewiesen.

![alt text](/Bilder/image_202.png)

### 2. Schritt – Cloud-Init Script eingebunden

Beim Erstellen der Instanzen habe ich das folgende Cloud-Init Script eingebunden. Dieses installiert Docker und konfiguriert den Host:

````
#cloud-config
packages:
  - apt-transport-https
  - ca-certificates
  - curl
  - gnupg-agent
  - software-properties-common
write_files:
  - path: /etc/sysctl.d/enabled_ipv4_forwarding.conf
    content: |
      net.ipv4.conf.all.forwarding=1
groups:
  - docker
system_info:
  default_user:
    groups: [docker]
runcmd:
  - curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
  - add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
  - apt-get update -y
  - apt-get install -y docker-ce docker-ce-cli containerd.io
  - systemctl start docker
  - systemctl enable docker
  - apt-get install podman -y
  - systemctl start podman
  - systemctl enable podman
````

### 3. Schritt – Docker Swarm initialisieren (Manager 1)

Ich habe mich via SSH auf den ersten Manager-Node verbunden und mit folgendem Befehl den Swarm initialisiert:

````
docker swarm init --advertise-addr <PRIVATE_IP_MANAGER_1>
````

![alt text](/Bilder/image_205.png)

### 4. Schritt – Manager & Worker Nodes joinen

Anschliesend habe ich die anderen Nodes (Manager 2, Manager 3, Worker 1 und Worker 2) mit den jeweiligen Join-Tokens hinzugefügt:

````
# Auf Manager 2 und 3
docker swarm join --token <MANAGER_TOKEN> <PRIVATE_IP_MANAGER_1>:2377
````
- Manager2
![alt text](/Bilder/image_213.png)

- Manager3 
![alt text](/Bilder/image_215.png)

# Auf Worker 1 und 2

````
docker swarm join --token <WORKER_TOKEN> <PRIVATE_IP_MANAGER_1>:2377
````
- Worker1
![alt text](/Bilder/image_216.png)

- Worker2
![alt text](/Bilder/image_217.png)

### 5. Schritt – Unbrauchbaren Node entfernen

Ein veralteter Manager-Node mit Status "Down" wurde zuerst demoted:



### 6. Schritt – Manager-Nodes auf Drain setzen

Damit nur die Worker-Nodes Container zugewiesen bekommen, habe ich die Availability der Manager-Nodes auf Drain gesetzt. Das verhindert, dass sie produktive Container übernehmen – ein Best Practice in produktiven Umgebungen.

![alt text](/Bilder/image_218.png)

````
docker node update --availability drain ip-172-31-93-217
docker node update --availability drain ip-172-31-80-191
docker node update --availability drain ip-172-31-92-183
````

![alt text](/Bilder/image_219.png)


### 7. Schritt – Node-Status überprüfen

Auf dem ersten Manager habe ich den Cluster-Status überprüft:

````
docker node ls
````

Ergebnis:

- 3 Manager-Nodes (Leader + 2 Reachable)
- 2 Worker-Nodes (Status: Active)

Keine Nodes im Status Down oder Unreachable

![alt text](/Bilder/image_212.png)

---